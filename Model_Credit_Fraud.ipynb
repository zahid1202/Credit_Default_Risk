{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from functools import reduce\n",
    "import re as re\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import warnings\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import  OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multilabel_train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"DataSet/application_train.csv\")\n",
    "test = pd.read_csv(\"DataSet/application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 14.60 MB\n",
      "Decreased by 67.6%\n",
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n"
     ]
    }
   ],
   "source": [
    "test = reduce_mem(test)\n",
    "train = reduce_mem(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I realize the CODE_GENDER==\"XNA\" (only 4 observations with TARGET of 0) doesn't show up in the test data, but it's messing up with the label encoder, so I droped them all along.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(index = train.loc[train.CODE_GENDER==\"XNA\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These 4 columns (['NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'EMERGENCYSTATE_MODE']) are label encoded\n",
      "Dimensions before OneHotEncoding : (307511, 122)\n",
      "Dimensions after OneHotEncoding : (307511, 242)\n",
      "These 5 columns (['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'EMERGENCYSTATE_MODE']) are label encoded\n",
      "Dimensions before OneHotEncoding : (48744, 121)\n",
      "Dimensions after OneHotEncoding : (48744, 237)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>HOUSETYPE_MODE_block of flats</th>\n",
       "      <th>HOUSETYPE_MODE_specific housing</th>\n",
       "      <th>HOUSETYPE_MODE_terraced house</th>\n",
       "      <th>WALLSMATERIAL_MODE_Block</th>\n",
       "      <th>WALLSMATERIAL_MODE_Mixed</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Others</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>WALLSMATERIAL_MODE_Wooden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0      100001                   0            0             0                1   \n",
       "1      100005                   0            1             0                1   \n",
       "2      100013                   0            1             1                1   \n",
       "3      100028                   0            0             0                1   \n",
       "4      100038                   0            1             1                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          135000.0    568800.0      20560.5         450000.0   \n",
       "1             0           99000.0    222768.0      17370.0         180000.0   \n",
       "2             0          202500.0    663264.0      69777.0         630000.0   \n",
       "3             2          315000.0   1575000.0      49018.5        1575000.0   \n",
       "4             1          180000.0    625500.0      32067.0         625500.0   \n",
       "\n",
       "             ...              HOUSETYPE_MODE_block of flats  \\\n",
       "0            ...                                          1   \n",
       "1            ...                                          0   \n",
       "2            ...                                          0   \n",
       "3            ...                                          1   \n",
       "4            ...                                          0   \n",
       "\n",
       "   HOUSETYPE_MODE_specific housing  HOUSETYPE_MODE_terraced house  \\\n",
       "0                                0                              0   \n",
       "1                                0                              0   \n",
       "2                                0                              0   \n",
       "3                                0                              0   \n",
       "4                                0                              0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Block  WALLSMATERIAL_MODE_Mixed  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Monolithic  WALLSMATERIAL_MODE_Others  \\\n",
       "0                              0                          0   \n",
       "1                              0                          0   \n",
       "2                              0                          0   \n",
       "3                              0                          0   \n",
       "4                              0                          0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Panel  WALLSMATERIAL_MODE_Stone, brick  \\\n",
       "0                         0                                1   \n",
       "1                         0                                0   \n",
       "2                         0                                0   \n",
       "3                         1                                0   \n",
       "4                         0                                0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Wooden  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(df):\n",
    "    le = LabelEncoder()\n",
    "    oe = OneHotEncoder()\n",
    "    le_col=[]\n",
    "    for col in df:\n",
    "        if df[col].dtype ==\"object\":\n",
    "            if df[col].nunique()==2:\n",
    "                df[col].replace(np.nan, \"NAN\", inplace=True)\n",
    "                le_col.append(col)\n",
    "                le.fit(df[col])\n",
    "                df[col] = le.transform(df[col])\n",
    "    print(\"These {} columns ({}) are label encoded\".format(len(le_col), le_col))\n",
    "    print(\"Dimensions before OneHotEncoding : {}\".format(df.shape))\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"Dimensions after OneHotEncoding : {}\".format(df.shape))\n",
    "    \n",
    "    return df\n",
    "                \n",
    "train =encode(train)\n",
    "train.head()\n",
    "test = encode(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of train data before alinging: (307511, 242) \n",
      " Shape of test data before alinging: (48744, 237)\n",
      " Shape of train data after alinging: (307511, 237) \n",
      " Shape of test data after alinging: (48744, 236)\n"
     ]
    }
   ],
   "source": [
    "def align(train, test):\n",
    "    \n",
    "    print(\" Shape of train data before alinging: {}\".format(train.shape),\"\\n\", \n",
    "         \"Shape of test data before alinging: {}\".format(test.shape))\n",
    "    target = train[\"TARGET\"]\n",
    "    train, test = train.align(test, join=\"inner\", axis =1)\n",
    "    train[\"TARGET\"] = target\n",
    "    print(\" Shape of train data after alinging: {}\".format(train.shape), \"\\n\", \n",
    "     \"Shape of test data after alinging: {}\".format(test.shape))\n",
    "    return train, test\n",
    "train, test = align(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_correction(df, feature = \"DAYS_EMPLOYED\", integer=365243):\n",
    "    df['Birth_ANOMALY'] = (df[feature]==integer).astype(int)\n",
    "\n",
    "    df[feature].replace({integer: np.nan}, inplace =True )\n",
    "    return df\n",
    "train = anomaly_correction(train, feature = \"DAYS_EMPLOYED\", integer=365243)\n",
    "test = anomaly_correction(test, feature = \"DAYS_EMPLOYED\", integer=365243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the negative values for some features, which should be positive, into positive.\n",
    "train[[\"DAYS_EMPLOYED\",\"DAYS_BIRTH\"]] = train[[\"DAYS_EMPLOYED\",\"DAYS_BIRTH\"]].apply(lambda x: x*(-1))\n",
    "test[[\"DAYS_EMPLOYED\",\"DAYS_BIRTH\"]]= test[[\"DAYS_EMPLOYED\",\"DAYS_BIRTH\"]].apply(lambda x: x*(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatively correlated features are:\n",
      " EXT_SOURCE_3                           -0.178925\n",
      "EXT_SOURCE_2                           -0.160471\n",
      "EXT_SOURCE_1                           -0.155318\n",
      "DAYS_BIRTH                             -0.078242\n",
      "DAYS_EMPLOYED                          -0.074957\n",
      "NAME_EDUCATION_TYPE_Higher education   -0.056593\n",
      "NAME_INCOME_TYPE_Pensioner             -0.046211\n",
      "ORGANIZATION_TYPE_XNA                  -0.045989\n",
      "Birth_ANOMALY                          -0.045989\n",
      "FLOORSMAX_AVG                          -0.044010\n",
      "FLOORSMAX_MEDI                         -0.043775\n",
      "FLOORSMAX_MODE                         -0.043233\n",
      "HOUSETYPE_MODE_block of flats          -0.040592\n",
      "AMT_GOODS_PRICE                        -0.039647\n",
      "EMERGENCYSTATE_MODE                    -0.039408\n",
      "REGION_POPULATION_RELATIVE             -0.037229\n",
      "ELEVATORS_AVG                          -0.034202\n",
      "ELEVATORS_MEDI                         -0.033866\n",
      "FLOORSMIN_AVG                          -0.033620\n",
      "FLOORSMIN_MEDI                         -0.033401\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr = train.corr()['TARGET'].sort_values()\n",
    "print(\"Negatively correlated features are:\\n\",corr.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positively correlated features are:\n",
      " NAME_HOUSING_TYPE_With parents                       0.029965\n",
      "OCCUPATION_TYPE_Drivers                              0.030303\n",
      "DEF_60_CNT_SOCIAL_CIRCLE                             0.031295\n",
      "DEF_30_CNT_SOCIAL_CIRCLE                             0.032261\n",
      "LIVE_CITY_NOT_WORK_CITY                              0.032517\n",
      "OWN_CAR_AGE                                          0.037611\n",
      "DAYS_REGISTRATION                                    0.041976\n",
      "OCCUPATION_TYPE_Laborers                             0.043017\n",
      "FLAG_DOCUMENT_3                                      0.044341\n",
      "REG_CITY_NOT_LIVE_CITY                               0.044394\n",
      "FLAG_EMP_PHONE                                       0.045984\n",
      "NAME_EDUCATION_TYPE_Secondary / secondary special    0.049822\n",
      "REG_CITY_NOT_WORK_CITY                               0.050992\n",
      "DAYS_ID_PUBLISH                                      0.051457\n",
      "CODE_GENDER                                          0.054710\n",
      "DAYS_LAST_PHONE_CHANGE                               0.055220\n",
      "NAME_INCOME_TYPE_Working                             0.057483\n",
      "REGION_RATING_CLIENT                                 0.058901\n",
      "REGION_RATING_CLIENT_W_CITY                          0.060895\n",
      "TARGET                                               1.000000\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Positively correlated features are:\\n\",corr.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Engineered Treaining Data: (307511, 219)\n",
      "Shape of the Engineered Test Data: (48744, 219)\n",
      "First 15 Engineered Features: ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE_Higher education', 'ORGANIZATION_TYPE_XNA', 'Birth_ANOMALY', 'FLOORSMAX_AVG', 'EXT_SOURCE_1^2', 'EXT_SOURCE_1 EXT_SOURCE_2', 'EXT_SOURCE_1 EXT_SOURCE_3', 'EXT_SOURCE_1 DAYS_BIRTH', 'EXT_SOURCE_1 DAYS_EMPLOYED', 'EXT_SOURCE_1 NAME_EDUCATION_TYPE_Higher education']\n",
      " Shape of train data before alinging: (307511, 457) \n",
      " Shape of test data before alinging: (48744, 456)\n",
      " Shape of train data after alinging: (307511, 457) \n",
      " Shape of test data after alinging: (48744, 456)\n"
     ]
    }
   ],
   "source": [
    "def features_engd(features, df_train, df_test, degree):\n",
    "    \"\"\" Features are given as a list.\"\"\"\n",
    "    df_train_eng = df_train[features]\n",
    "    df_test_eng = df_test[features]\n",
    "    \n",
    "    # Imputations\n",
    "    imputer = Imputer(strategy='mean')\n",
    "    # Instantiating polynomialfeaures\n",
    "    pf = PolynomialFeatures(degree, include_bias =False)\n",
    "    \n",
    "    df_train_eng =imputer.fit_transform(df_train_eng)\n",
    "    df_train_eng =pf.fit_transform(df_train_eng)\n",
    "    \n",
    "    df_test_eng =imputer.transform(df_test_eng)\n",
    "    df_test_eng =pf.transform(df_test_eng)\n",
    "    print(\"Shape of the Engineered Treaining Data: {}\".format(df_train_eng.shape))\n",
    "    print(\"Shape of the Engineered Test Data: {}\".format(df_test_eng.shape))\n",
    "    print(\"First 15 Engineered Features: {}\".format(pf.get_feature_names(input_features = features)[:15]))\n",
    "    \n",
    "    # Converting the engineered features into a data frame\n",
    "    df_train_eng = pd.DataFrame(df_train_eng, columns=pf.get_feature_names(input_features = features))\n",
    "    df_test_eng = pd.DataFrame(df_test_eng, columns=pf.get_feature_names(input_features = features))\n",
    "\n",
    "        \n",
    "    # Now merging the engineered dataframes\n",
    "    df_train_eng[\"SK_ID_CURR\"] =df_train[\"SK_ID_CURR\"]\n",
    "    df_test_eng[\"SK_ID_CURR\"] =df_test[\"SK_ID_CURR\"]\n",
    "    \n",
    "    \n",
    "    df_train = df_train.merge(df_train_eng, how= \"left\", on = \"SK_ID_CURR\")\n",
    "    df_test = df_test.merge(df_test_eng, how= \"left\", on = \"SK_ID_CURR\")\n",
    "        \n",
    "    # Aligning the engineered train and test data\n",
    "    if df_train.shape != df_test.shape:\n",
    "        df_train, df_test = align(df_train, df_test)\n",
    "    \n",
    "    # Adding the \"TARGET\" features to the engineered dataframe \n",
    "    df_train_eng[\"TARGET\"] =df_train[\"TARGET\"]\n",
    "    \n",
    "    return df_train_eng, df_test_eng, df_train, df_test\n",
    "\n",
    "features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "            \"DAYS_BIRTH\",\"DAYS_EMPLOYED\", \"NAME_EDUCATION_TYPE_Higher education\",\n",
    "            \"ORGANIZATION_TYPE_XNA\", \"Birth_ANOMALY\",\"FLOORSMAX_AVG\"]\n",
    "\n",
    "train_eng, test_eng, df_eng_train, df_eng_test = features_engd(features, train, test, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Engineered Treaining Data: (307511, 55)\n",
      "Shape of the Engineered Test Data: (48744, 55)\n",
      "First 15 Engineered Features: ['EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH', 'EXT_SOURCE_2^2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3^2', 'EXT_SOURCE_2 EXT_SOURCE_3^2', 'EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH', 'EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2^2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2 EXT_SOURCE_3^2', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3^2', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2^2 EXT_SOURCE_3', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 EXT_SOURCE_2 EXT_SOURCE_3^2', 'EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH^2']\n",
      " Shape of train data before alinging: (307511, 512) \n",
      " Shape of test data before alinging: (48744, 511)\n",
      " Shape of train data after alinging: (307511, 514) \n",
      " Shape of test data after alinging: (48744, 513)\n"
     ]
    }
   ],
   "source": [
    "eng_fts = [\"EXT_SOURCE_2 EXT_SOURCE_3\",\"EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3\",\n",
    "           \"EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH\", \"EXT_SOURCE_2^2 EXT_SOURCE_3\", \"EXT_SOURCE_2 EXT_SOURCE_3^2\"]\n",
    "df_train_eng2, df_test_eng2, eng_train2, eng_test2 = features_engd(eng_fts, eng_train, eng_test, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(train, test, file_name= \"log_reg\", C=1):\n",
    "# Here I am building a pipeline\n",
    "    test = train[\"TARGET\"].values\n",
    "    train = train.drop(\"TARGET\", axis=1).values\n",
    "\n",
    "    test = pd.get_dummies(test).values\n",
    "    pl = Pipeline([\n",
    "        (\"imputaton\", Imputer(strategy=\"median\")),\n",
    "        (\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "        (\"clf\", OneVsRestClassifier(estimator=LogisticRegression(C=C,class_weight=None,\n",
    "                                                                 dual=False, fit_intercept=True,\n",
    "                                                                 intercept_scaling=1, max_iter=100,\n",
    "                                                                 multi_class='ovr', n_jobs=1, \n",
    "                                                                 penalty='l2', random_state=4,\n",
    "                                                                 solver='liblinear', tol=0.0001, \n",
    "                                                                 verbose=0, warm_start=False)))\n",
    "    ])\n",
    "    pl.fit(trn,trt)\n",
    "    proba = pl.predict_proba(test)\n",
    "    submission = test[[\"SK_ID_CURR\"]]\n",
    "    submission[\"TARGET\"]  =proba[:,1]\n",
    "    submission = submission.to_csv(file_name + '.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_rocauc(train, C=1):\n",
    "# Here I am building a pipeline\n",
    "    test = train[\"TARGET\"].values\n",
    "    train = train.drop(\"TARGET\", axis=1).values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train,test,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=test,\n",
    "                                                        random_state= 42)\n",
    "    #trt = pd.get_dummies(trt).values\n",
    "    pl = Pipeline([\n",
    "        (\"imputaton\", Imputer(strategy=\"median\")),\n",
    "        (\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "        (\"clf\", OneVsRestClassifier(estimator=LogisticRegression(C=C,class_weight=None,\n",
    "                                                                 dual=False, fit_intercept=True,\n",
    "                                                                 intercept_scaling=1, max_iter=100,\n",
    "                                                                 multi_class='ovr', n_jobs=1, \n",
    "                                                                 penalty='l2', random_state=4,\n",
    "                                                                 solver='liblinear', tol=0.0001, \n",
    "                                                                 verbose=0, warm_start=False)))\n",
    "    ])\n",
    "    pl.fit(X_train, y_train)\n",
    "    y_proba = pl.predict_proba(X_test)[:,1]\n",
    "    rocauc = roc_auc_score(y_test, y_proba)\n",
    "    print(\"ROC_AUC Score is {}\".format(rocauc))\n",
    "    return rocauc\n",
    "    #submission = test[[\"SK_ID_CURR\"]]\n",
    "    #submission[\"TARGET\"]  =proba[:,1]\n",
    "    #submission = submission.to_csv(file_name + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC Score is 0.7510089748120818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7510089748120818"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_rocauc(eng_train, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC Score is 0.7510514312629597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7510514312629597"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_rocauc(eng_train2, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def decision_tree_rocauc(train):\n",
    "    test = train[\"TARGET\"].values\n",
    "    train = train.drop(\"TARGET\", axis=1).values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train,test,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=test,\n",
    "                                                        random_state= 42)\n",
    "    # Instantiating a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "    dt = DecisionTreeClassifier(max_depth =6,criterion=\"entropy\", random_state=43)\n",
    "    \n",
    "    pl = Pipeline([\n",
    "    (\"imputaton\", Imputer(strategy=\"median\")),\n",
    "    (\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "    (\"clf\", OneVsRestClassifier(estimator=dt))\n",
    "    ])\n",
    "    \n",
    "    pl.fit(X_train, y_train)\n",
    "    y_proba = pl.predict_proba(X_test)[:,1]\n",
    "    rocauc = roc_auc_score(y_test, y_proba)\n",
    "    print(\"ROC_AUC Score is {}\".format(rocauc))\n",
    "    return y_proba\n",
    "\n",
    "# Diagnosing bias-variance problem: to see how well our model fits data(i.e. if our model underfits or overfits the data)\n",
    "\n",
    "def diagnose_bias_varaince(train):\n",
    "    imputer= Imputer(strategy =\"mean\")\n",
    "\n",
    "    \n",
    "    test = train[\"TARGET\"]\n",
    "    train = train.drop(\"TARGET\", axis=1)\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train,test,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=test,\n",
    "                                                        random_state= 42)\n",
    "    pl = Pipeline([\n",
    "    (\"imputaton\", Imputer(strategy=\"median\")),\n",
    "    (\"scale\", MinMaxScaler(feature_range=(0, 1)))\n",
    "    ])\n",
    "    \n",
    "    X_train = pl.fit_transform(X_train)\n",
    "    X_test = pl.transform(X_test)\n",
    "    \n",
    "    # Instantiating a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "    \n",
    "    dt = DecisionTreeClassifier(max_depth =19,criterion=\"gini\",\n",
    "                                 random_state=43, class_weight =\"balanced\")\n",
    "    # Calculate cv scores\n",
    "    ROC_CV_scores = cross_val_score(dt, X_train, y_train, cv=2, \n",
    "                                  scoring='roc_auc', \n",
    "                                  n_jobs=-1)\n",
    "    #min_samples_leaf=0.1,\n",
    "    # Compute the 10-folds ROC_CV\n",
    "    ROC_CV = ROC_CV_scores\n",
    "    ROC_CV_mean = ROC_CV_scores.mean()\n",
    "\n",
    "    # Print ROC_CV\n",
    "    print('ROC_CV_mean: {:.2f}'.format(ROC_CV_mean))\n",
    "    \n",
    "    # Fit dt to the training set\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the training set\n",
    "    y_pred = dt.predict(X_train)\n",
    "    \n",
    "    ROC_train = roc_auc_score(y_train, y_pred)\n",
    "    print(\"ROC_train: {:.2f}\".format(ROC_train))\n",
    "    \n",
    "    #y_proba = pl.predict_proba(X_test)[:,1]\n",
    "\n",
    "    return ROC_CV, ROC_CV_mean, ROC_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_CV_mean: 0.69\n",
      "ROC_train: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.68965739, 0.68310577, 0.68644907, 0.6740724 , 0.67802664,\n",
       "        0.69459941, 0.69250055, 0.68221918, 0.69067273, 0.69700731]),\n",
       " 0.6868310462260004,\n",
       " 0.5)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_depth =7,criterion=\"gini\",#min_samples_leaf=0.26,\n",
    "diagnose_bias_varaince(df_eng_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our model overfitted the data!\n",
    "\n",
    "    Over fitting occurs when the model captures the noise and the outliers in the data along with the underlying pattern. These models usually have high variance and low bias. These models are usually complex like Decision Trees, SVM or Neural Networks which are prone to over fitting.\n",
    "\n",
    "    Under fitting occurs when the model is unable to capture the underlying pattern of the data. These models usually have a low variance and a high bias. These models are usually simple which are unable to capture the complex patterns in the data like Linear and Logistic Regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_CV_mean: 0.69\n",
      "ROC_train: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.68965739, 0.68310577, 0.68644907, 0.6740724 , 0.67802664,\n",
       "        0.69459941, 0.69250055, 0.68221918, 0.69067273, 0.69700731]),\n",
       " 0.6868310462260004,\n",
       " 0.5)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's decrease the max_depth\n",
    "#max_depth =5,criterion=\"gini\",#min_samples_leaf=0.26,\n",
    "diagnose_bias_varaince(df_eng_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_CV_mean: 0.69\n",
      "ROC_train: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.68965739, 0.68310577, 0.68644907, 0.6740724 , 0.67802664,\n",
       "        0.69459941, 0.69250055, 0.68221918, 0.69067273, 0.69700731]),\n",
       " 0.6868310462260004,\n",
       " 0.5)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's decrease the max_depth\n",
    "#max_depth =12,criterion=\"gini\",#min_samples_leaf=0.26,\n",
    "diagnose_bias_varaince(df_eng_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's decrease the max_depth\n",
    "#max_depth =3,criterion=\"gini\",#min_samples_leaf=0.26,\n",
    "diagnose_bias_varaince(df_eng_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
